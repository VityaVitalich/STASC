{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_rationale_and_final_answer(generated_text: str):\n",
    "    \"\"\"\n",
    "    Given a text that follows the pattern:\n",
    "      \"Step-by-step reasoning:\\n...some rationale...\\nFinal Answer:\\n...the answer...\"\n",
    "    return a (rationale, final_answer) tuple.\n",
    "\n",
    "    If the labels aren't found, defaults to empty strings.\n",
    "    \"\"\"\n",
    "    # Label markers used in your STaR prompts\n",
    "    rationale_marker = \"Step-by-step reasoning:\"\n",
    "    answer_marker = \"Final Answer:\"\n",
    "\n",
    "    # Initialize outputs\n",
    "    rationale = \"\"\n",
    "    final_ans = \"\"\n",
    "\n",
    "    # Normalize line breaks (optional)\n",
    "    text = generated_text.replace(\"\\r\", \"\")\n",
    "\n",
    "    # Locate the markers\n",
    "    rationale_start_idx = text.find(rationale_marker)\n",
    "    answer_start_idx = text.find(answer_marker)\n",
    "\n",
    "    if rationale_start_idx != -1:\n",
    "        # Move index to start of actual rationale text (beyond the marker)\n",
    "        rationale_start = rationale_start_idx + len(rationale_marker)\n",
    "        if answer_start_idx != -1 and answer_start_idx > rationale_start:\n",
    "            # Rationale is everything from rationale_start up to the \"Final Answer:\" marker\n",
    "            rationale = text[rationale_start:answer_start_idx].strip()\n",
    "        else:\n",
    "            # If \"Final Answer:\" not found, or it's before rationale_start, treat the rest as rationale\n",
    "            rationale = text[rationale_start:].strip()\n",
    "\n",
    "    if answer_start_idx != -1:\n",
    "        # Move index to start of final answer\n",
    "        answer_start = answer_start_idx + len(answer_marker)\n",
    "        final_ans = text[answer_start:].strip()\n",
    "\n",
    "    return rationale, final_ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rationale: First, realize it is in Paris.\n",
      "Check that it's in France.\n",
      "Final: The Eiffel Tower is in Paris, France.\n"
     ]
    }
   ],
   "source": [
    "# Suppose you have an LLM response:\n",
    "response_text = (\n",
    "    \"Step-by-step reasoning:\\n\"\n",
    "    \"First, realize it is in Paris.\\n\"\n",
    "    \"Check that it's in France.\\n\"\n",
    "    \"Final Answer:\\n\"\n",
    "    \"The Eiffel Tower is in Paris, France.\"\n",
    ")\n",
    "\n",
    "rationale_part, final_part = split_rationale_and_final_answer(response_text)\n",
    "print(\"Rationale:\", rationale_part)\n",
    "print(\"Final:\", final_part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_answers_into_rows(\n",
    "    examples,\n",
    "    question_col=\"question\",\n",
    "    answers_col=\"model_outputs\",\n",
    "    id_col=\"id\",\n",
    "    new_id_col=\"new_id\",\n",
    "    new_question_col=\"question\",\n",
    "    new_answer_col=\"answer\"\n",
    "):\n",
    "    \"\"\"\n",
    "    This function will be called in batched mode by dataset.map(..., batched=True).\n",
    "    `examples` is a dictionary of lists, e.g.:\n",
    "       {\n",
    "         \"id\": [id1, id2, ...],\n",
    "         \"question\": [q1, q2, ...],\n",
    "         \"model_outputs\": [[ans11, ans12], [ans21, ans22, ans23], ...]\n",
    "       }\n",
    "\n",
    "    We want to \"explode\" each list of answers into multiple rows.\n",
    "\n",
    "    Return a dict of lists:\n",
    "       {\n",
    "         new_id_col: [...],\n",
    "         new_question_col: [...],\n",
    "         new_answer_col: [...]\n",
    "       }\n",
    "    so huggingface Dataset can expand them properly.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Extract the entire batch columns as lists\n",
    "    batch_ids = examples[id_col]\n",
    "    batch_questions = examples[question_col]\n",
    "    batch_answers = examples[answers_col]\n",
    "\n",
    "    # Prepare output lists\n",
    "    out_new_ids = []\n",
    "    out_questions = []\n",
    "    out_answers = []\n",
    "\n",
    "    # 2) Iterate over each row in this batch\n",
    "    for i in range(len(batch_ids)):\n",
    "        original_id = str(batch_ids[i])\n",
    "        question_text = batch_questions[i]\n",
    "        answers_list = batch_answers[i]\n",
    "\n",
    "        # If it's not a list, wrap in a list\n",
    "        if not isinstance(answers_list, list):\n",
    "            answers_list = [answers_list]\n",
    "\n",
    "        # 3) \"Explode\" each answer into a new row\n",
    "        for idx_ans, ans in enumerate(answers_list):\n",
    "            new_id = f\"{original_id}_{idx_ans}\"\n",
    "            out_new_ids.append(new_id)\n",
    "            out_questions.append(question_text)\n",
    "            out_answers.append(ans)\n",
    "\n",
    "    # 4) Return dict of lists (the new \"expanded\" columns)\n",
    "    return {\n",
    "        new_id_col: out_new_ids,\n",
    "        new_question_col: out_questions,\n",
    "        new_answer_col: out_answers\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploded Dataset:\n",
      "{'new_id': ['rowA_0', 'rowA_1', 'rowA_2', 'rowB_0', 'rowB_1'], 'question': ['Where is the Eiffel Tower?', 'Where is the Eiffel Tower?', 'Where is the Eiffel Tower?', 'Who was the first president of the US?', 'Who was the first president of the US?'], 'answer': ['Paris', 'France', 'On Earth', 'George Washington', 'John Adams']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Apply the explode function\n",
    "exploded_dataset = explode_answers_into_rows(original_dataset)\n",
    "\n",
    "print(\"Exploded Dataset:\")\n",
    "print(exploded_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['new_id', 'question', 'answer'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.from_dict(exploded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # 4) Verify the result\n",
    "    #    We expect 3 + 2 = 5 rows total\n",
    "    assert len(exploded_dataset) == 5, \"Expected 5 rows in exploded dataset.\"\n",
    "    #    Let's check a few row samples:\n",
    "    row0 = exploded_dataset[0]\n",
    "    assert row0[\"new_id\"] == \"rowA_0\", \"Unexpected ID for first exploded row.\"\n",
    "    assert row0[\"question\"] == \"Where is the Eiffel Tower?\", \"Question mismatch\"\n",
    "    assert row0[\"answer\"] == \"Paris\", \"Answer mismatch\"\n",
    "\n",
    "    row3 = exploded_dataset[3]  # first row of second question\n",
    "    assert row3[\"new_id\"] == \"rowB_0\", \"Unexpected ID for rowB_0.\"\n",
    "    assert row3[\"answer\"] == \"George Washington\", \"Answer mismatch for rowB_0\"\n",
    "\n",
    "    print(\"All tests passed! :)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "Dataset({\n",
      "    features: ['id', 'question', 'model_outputs'],\n",
      "    num_rows: 2\n",
      "})\n",
      "     id                                question  \\\n",
      "0  rowA              Where is the Eiffel Tower?   \n",
      "1  rowB  Who was the first president of the US?   \n",
      "\n",
      "                     model_outputs  \n",
      "0        [Paris, France, On Earth]  \n",
      "1  [George Washington, John Adams]   \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74dd064640b94442a6180827e4a6ad21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Provided `function` which is applied to all elements of table returns a variable of type <class 'list'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_explode_answers_into_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 60\u001b[0m, in \u001b[0;36mtest_explode_answers_into_rows\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exploded_ds\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# 3) Apply the explode function\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m exploded_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mexplode_answers_into_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExploded Dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(exploded_dataset)\n",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m, in \u001b[0;36mtest_explode_answers_into_rows.<locals>.explode_answers_into_rows\u001b[0;34m(dataset, question_col, answers_col, new_id_col, new_answer_col)\u001b[0m\n\u001b[1;32m     45\u001b[0m         new_rows\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     46\u001b[0m             new_id_col: new_id_val,\n\u001b[1;32m     47\u001b[0m             question_col: q,\n\u001b[1;32m     48\u001b[0m             new_answer_col: ans\n\u001b[1;32m     49\u001b[0m         })\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_rows\n\u001b[0;32m---> 52\u001b[0m exploded_ds \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplode_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# remove original columns\u001b[39;49;00m\n\u001b[1;32m     56\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exploded_ds\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:3035\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3031\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3032\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3033\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3034\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3035\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3036\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3037\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:3408\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3406\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3407\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3408\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3410\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:3316\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3314\u001b[0m         updatable_types \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pl\u001b[38;5;241m.\u001b[39mDataFrame,)\n\u001b[1;32m   3315\u001b[0m     update_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, updatable_types)\n\u001b[0;32m-> 3316\u001b[0m     \u001b[43mvalidate_function_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m update_data:\n\u001b[1;32m   3318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Nothing to update, let's move on\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:3252\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.validate_function_output\u001b[0;34m(processed_inputs, indices)\u001b[0m\n\u001b[1;32m   3250\u001b[0m     allowed_processed_inputs_types \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pl\u001b[38;5;241m.\u001b[39mDataFrame,)\n\u001b[1;32m   3251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processed_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, allowed_processed_inputs_types):\n\u001b[0;32m-> 3252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3253\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `function` which is applied to all elements of table returns a variable of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(processed_inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3254\u001b[0m     )\n\u001b[1;32m   3255\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indices, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, Mapping):\n\u001b[1;32m   3256\u001b[0m     allowed_batch_return_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, pd\u001b[38;5;241m.\u001b[39mSeries)\n",
      "\u001b[0;31mTypeError\u001b[0m: Provided `function` which is applied to all elements of table returns a variable of type <class 'list'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects."
     ]
    }
   ],
   "source": [
    "test_explode_answers_into_rows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
