# Model Configuration
model_path: ""
model_name_short: ""
gpu_memory_utilization: 0.8 # GPU memory utilization (0.0 to 1.0)
enforce_eager: True # Whether to enforce eager execution
max_model_len: 4096 # Maximum model length
random_seed: 42

# Sampling Parameters
temperature: 0.7 # Sampling temperature
top_p: 0.8 # Top-p (nucleus) sampling
max_tokens: 2048 # Maximum number of tokens to generate
trust_remote_code: false
use_fast_tokenizer: false
torch_dtype: bfloat16
model_revision: main
