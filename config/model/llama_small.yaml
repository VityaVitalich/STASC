# Model Configuration
model_name: "meta-llama/Llama-3.2-1B-Instruct"
model_name_short: "llama3.2-1B-instruct"
cache_dir: "/home/data/v.moskvoretskii/cache/"
gpu_memory_utilization: 0.95 # GPU memory utilization (0.0 to 1.0)
enforce_eager: False # Whether to enforce eager execution
max_model_len: 4096 # Maximum model length
random_seed: 42

# Sampling Parameters
temperature: 0 # Sampling temperature
top_p: 0.001 # Top-p (nucleus) sampling
max_tokens: 2048 # Maximum number of tokens to generate
